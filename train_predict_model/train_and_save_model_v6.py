"""
3D ì¢Œí‘œ ê¸°ë°˜ ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (npy íŒŒì¼ ì‚¬ìš©)
3d_coordinate_extractorë¡œ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë°ì´í„° êµ¬ì¡°:
coordinates_output/
  â”œâ”€â”€ metadata.csv  (id, label ì •ë³´ í¬í•¨)
  â”œâ”€â”€ L_001_keypoints.npy
  â”œâ”€â”€ L_002_keypoints.npy
  â””â”€â”€ ...

ì‚¬ìš©ë²•:
# ê¸°ë³¸ í•™ìŠµ
python train_and_save_model_v6.py --data-dir coordinates_output

# ë°ì´í„° ì¦ê°• ì ìš©
python train_and_save_model_v6.py --data-dir coordinates_output --augment

# ì—í¬í¬ ë° ë°°ì¹˜ í¬ê¸° ì¡°ì •
python train_and_save_model_v6.py --data-dir coordinates_output --epochs 150 --batch-size 8

# ì €ì¥ ê²½ë¡œ ì§€ì •
python train_and_save_model_v6.py --data-dir coordinates_output --output-dir trained_model/my_model
"""

import numpy as np
import pickle
import argparse
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    LSTM, Bidirectional, Dense, Dropout, BatchNormalization,
    Masking, GRU
)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import (
    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard
)
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from datetime import datetime
import json
import pandas as pd

# seabornì€ ì„ íƒì  ì˜ì¡´ì„±
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    print("Warning: seaborn not found. Using matplotlib only for visualizations.")


class DataAugmentor:
    """ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    
    @staticmethod
    def augment_sequence(sequence: np.ndarray, aug_type: str = 'noise'):
        """
        ë°ì´í„° ì¦ê°•
        - noise: ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
        - speed: ì†ë„ ë³€í™” (ì‹œê°„ ì›Œí•‘)
        - scale: í¬ê¸° ë³€í™”
        """
        if aug_type == 'noise':
            # ì•½í•œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = np.random.normal(0, 0.02, sequence.shape)
            return sequence + noise
        
        elif aug_type == 'speed':
            # ì†ë„ ë³€í™” (0.8x ~ 1.2x)
            factor = np.random.uniform(0.8, 1.2)
            indices = np.linspace(0, len(sequence) - 1, int(len(sequence) * factor))
            indices = np.clip(indices, 0, len(sequence) - 1).astype(int)
            return sequence[indices]
        
        elif aug_type == 'scale':
            # í¬ê¸° ë³€í™” (0.9x ~ 1.1x)
            factor = np.random.uniform(0.9, 1.1)
            return sequence * factor
        
        return sequence


class SignLanguageTrainer:
    """ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (npy íŒŒì¼ + metadata.csv ê¸°ë°˜)"""
    
    def __init__(self, data_dir: str, output_dir: str = "trained_model/trained_model_v6"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.scaler = StandardScaler()
        self.augmentor = DataAugmentor()
        
        self.label_to_int = {}
        self.int_to_label = {}
        self.max_sequence_length = 0
        self.input_dim = 147  # 49ê°œ í‚¤í¬ì¸íŠ¸ Ã— 3 (x, y, z)
        
        # metadata.csv ë¡œë“œ
        self.metadata = self._load_metadata()
        
    def _load_metadata(self):
        """metadata.csv íŒŒì¼ ë¡œë“œ"""
        metadata_path = self.data_dir / "metadata.csv"
        
        if not metadata_path.exists():
            raise FileNotFoundError(
                f"metadata.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}\n"
                "3d_coordinate_extractor.pyë¡œ ìƒì„±ëœ metadata.csvê°€ í•„ìš”í•©ë‹ˆë‹¤."
            )
        
        df = pd.read_csv(metadata_path)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['id', 'label']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"metadata.csvì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
        
        print(f"metadata.csv ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
        
        return df
        
    def load_data(self, augment: bool = False):
        """metadata.csvë¥¼ ì°¸ì¡°í•˜ì—¬ npy íŒŒì¼ ë¡œë“œ"""
        print("\n" + "="*70)
        print("ë°ì´í„° ë¡œë”© ì‹œì‘ (metadata.csv ê¸°ë°˜)")
        print("="*70)
        
        sequences = []
        labels = []
        file_info = []
        
        # ê³ ìœ í•œ ë¼ë²¨ ì¶”ì¶œ ë° ë§¤í•‘ ìƒì„±
        unique_labels = sorted(self.metadata['label'].unique())
        for idx, label in enumerate(unique_labels):
            self.label_to_int[label] = idx
            self.int_to_label[idx] = label
        
        print(f"\në°œê²¬ëœ ìˆ˜ì–´ ë‹¨ì–´ ({len(self.label_to_int)}ê°œ):")
        for label, idx in self.label_to_int.items():
            count = len(self.metadata[self.metadata['label'] == label])
            print(f"  {idx}: {label} ({count}ê°œ íŒŒì¼)")
        
        # ê° íŒŒì¼ ë¡œë“œ
        print(f"\nì´ {len(self.metadata)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        
        success_count = 0
        failed_files = []
        
        for _, row in self.metadata.iterrows():
            file_id = row['id']
            label = row['label']
            
            # npy íŒŒì¼ ê²½ë¡œ ìƒì„± (id_keypoints.npy í˜•ì‹)
            npy_filename = f"{file_id}_keypoints.npy"
            npy_path = self.data_dir / npy_filename
            
            if not npy_path.exists():
                failed_files.append(f"{npy_filename} (íŒŒì¼ ì—†ìŒ)")
                continue
            
            try:
                # npy íŒŒì¼ ë¡œë“œ
                data = np.load(npy_path)
                
                if len(data) == 0:
                    failed_files.append(f"{npy_filename} (ë¹ˆ ë°ì´í„°)")
                    continue
                
                label_int = self.label_to_int[label]
                
                sequences.append(data)
                labels.append(label_int)
                file_info.append({
                    'file': npy_filename,
                    'id': file_id,
                    'label': label,
                    'frames': len(data)
                })
                success_count += 1
                
                # ë°ì´í„° ì¦ê°•
                if augment:
                    # ë…¸ì´ì¦ˆ ë²„ì „
                    aug_seq = self.augmentor.augment_sequence(data, 'noise')
                    sequences.append(aug_seq)
                    labels.append(label_int)
                    
                    # ì†ë„ ë³€í™” ë²„ì „
                    aug_seq = self.augmentor.augment_sequence(data, 'speed')
                    sequences.append(aug_seq)
                    labels.append(label_int)
                    
            except Exception as e:
                failed_files.append(f"{npy_filename} ({str(e)})")
        
        print(f"\në¡œë”© ê²°ê³¼:")
        print(f"  ì„±ê³µ: {success_count}ê°œ")
        print(f"  ì‹¤íŒ¨: {len(failed_files)}ê°œ")
        
        if failed_files and len(failed_files) <= 10:
            print(f"\nì‹¤íŒ¨í•œ íŒŒì¼:")
            for failed in failed_files:
                print(f"  - {failed}")
        elif len(failed_files) > 10:
            print(f"\nì‹¤íŒ¨í•œ íŒŒì¼ (ì²˜ìŒ 10ê°œ):")
            for failed in failed_files[:10]:
                print(f"  - {failed}")
            print(f"  ... ì™¸ {len(failed_files) - 10}ê°œ")
        
        if augment:
            print(f"\në°ì´í„° ì¦ê°• í›„: {len(sequences)}ê°œ ì‹œí€€ìŠ¤ (ì›ë³¸ Ã— 3)")
        else:
            print(f"\nì´ ì‹œí€€ìŠ¤: {len(sequences)}ê°œ")
        
        if len(sequences) == 0:
            raise ValueError("ë¡œë“œ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¼ë²¨ë³„ í†µê³„
        print(f"\në¼ë²¨ë³„ ë°ì´í„° ìˆ˜:")
        label_array = np.array(labels)
        for label, idx in sorted(self.label_to_int.items(), key=lambda x: x[1]):
            count = np.sum(label_array == idx)
            print(f"  {label}: {count}ê°œ")
        
        # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
        sequence_lengths = [len(seq) for seq in sequences]
        self.max_sequence_length = int(np.percentile(sequence_lengths, 95))
        print(f"\nì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„:")
        print(f"  95th percentile: {self.max_sequence_length} í”„ë ˆì„ (ëª¨ë¸ ì…ë ¥ ê¸¸ì´)")
        print(f"  í‰ê· : {np.mean(sequence_lengths):.1f} í”„ë ˆì„")
        print(f"  ì¤‘ì•™ê°’: {np.median(sequence_lengths):.1f} í”„ë ˆì„")
        print(f"  ìµœì†Œ/ìµœëŒ€: {min(sequence_lengths)}/{max(sequence_lengths)} í”„ë ˆì„")
        
        # íŒŒì¼ ì •ë³´ ì €ì¥
        if file_info:
            df_info = pd.DataFrame(file_info)
            info_path = self.output_dir / "training_data_info.csv"
            df_info.to_csv(info_path, index=False, encoding='utf-8-sig')
            print(f"\ní•™ìŠµ ë°ì´í„° ì •ë³´ ì €ì¥: {info_path}")
        
        return sequences, np.array(labels)
    
    def preprocess_data(self, sequences, labels):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• """
        print("\n" + "="*70)
        print("ë°ì´í„° ì „ì²˜ë¦¬")
        print("="*70)
        
        # ì‹œí€€ìŠ¤ í‰íƒ„í™” ë° ìŠ¤ì¼€ì¼ë§
        all_frames = []
        for seq in sequences:
            flattened = seq.reshape(seq.shape[0], -1)
            all_frames.append(flattened)
        
        all_frames = np.vstack(all_frames)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        print("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì¤‘...")
        self.scaler.fit(all_frames)
        print(f"  íŠ¹ì§• í‰ê· : {self.scaler.mean_[0]:.3f}, í‘œì¤€í¸ì°¨: {self.scaler.scale_[0]:.3f}")
        
        # ê° ì‹œí€€ìŠ¤ì— ìŠ¤ì¼€ì¼ë§ ë° íŒ¨ë”© ì ìš©
        processed_sequences = []
        for seq in sequences:
            flattened = seq.reshape(seq.shape[0], -1)
            scaled = self.scaler.transform(flattened)
            
            # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
            if scaled.shape[0] >= self.max_sequence_length:
                padded = scaled[:self.max_sequence_length]
            else:
                padding = np.zeros((self.max_sequence_length - scaled.shape[0], scaled.shape[1]))
                padded = np.vstack((scaled, padding))
            
            processed_sequences.append(padded)
        
        X = np.array(processed_sequences)
        y = to_categorical(labels, num_classes=len(self.label_to_int))
        
        print(f"\nì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  ì…ë ¥ shape: {X.shape}")
        print(f"  ì¶œë ¥ shape: {y.shape}")
        
        # Train/Test ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=labels
        )
        
        print(f"\në°ì´í„° ë¶„í• :")
        print(f"  í•™ìŠµ ë°ì´í„°: {X_train.shape[0]}ê°œ ({X_train.shape[0]/len(X)*100:.1f}%)")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ({X_test.shape[0]/len(X)*100:.1f}%)")
        
        # ë¶„í•  í›„ ë¼ë²¨ ë¶„í¬ í™•ì¸
        y_train_labels = np.argmax(y_train, axis=1)
        y_test_labels = np.argmax(y_test, axis=1)
        
        print(f"\ní•™ìŠµ ë°ì´í„° ë¼ë²¨ ë¶„í¬:")
        for label, idx in sorted(self.label_to_int.items(), key=lambda x: x[1]):
            train_count = np.sum(y_train_labels == idx)
            test_count = np.sum(y_test_labels == idx)
            print(f"  {label}: í•™ìŠµ {train_count}ê°œ, í…ŒìŠ¤íŠ¸ {test_count}ê°œ")
        
        return X_train, X_test, y_train, y_test
    
    def build_model(self):
        """ëª¨ë¸ êµ¬ì¶• (ê°•ë ¥í•œ ì •ê·œí™” ì ìš©)"""
        print("\n" + "="*70)
        print("ëª¨ë¸ êµ¬ì¶• (ê°•ë ¥í•œ ì •ê·œí™” ë²„ì „)")
        print("="*70)
        
        model = Sequential([
            # Masking layer (íŒ¨ë”©ëœ ë¶€ë¶„ ë¬´ì‹œ)
            Masking(mask_value=0.0, input_shape=(self.max_sequence_length, self.input_dim)),
            
            # ì²« ë²ˆì§¸ Bi-LSTM (í¬ê¸° ê°ì†Œ + dropout ì¦ê°€)
            Bidirectional(LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.3)),
            BatchNormalization(),
            
            # ë‘ ë²ˆì§¸ Bi-LSTM (í¬ê¸° ê°ì†Œ + dropout ì¦ê°€)
            Bidirectional(LSTM(32, return_sequences=False, dropout=0.5, recurrent_dropout=0.3)),
            BatchNormalization(),
            
            # Dense layers (L2 ì •ê·œí™” + dropout ì¦ê°€)
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.5),
            Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.5),
            
            # ì¶œë ¥ì¸µ
            Dense(len(self.label_to_int), activation='softmax')
        ])
        
        # ì»´íŒŒì¼ (í•™ìŠµë¥  ê°ì†Œ)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("\nëª¨ë¸ êµ¬ì¡°:")
        model.summary()
        
        total_params = model.count_params()
        print(f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"\nì •ê·œí™” ì„¤ì •:")
        print(f"  - LSTM í¬ê¸°: 128â†’64, 64â†’32")
        print(f"  - Dropout: 0.5 (LSTM), 0.5 (Dense)")
        print(f"  - Recurrent Dropout: 0.3")
        print(f"  - L2 ì •ê·œí™”: 0.01")
        print(f"  - í•™ìŠµë¥ : 0.001â†’0.0005")
        
        return model
    
    def train(self, model, X_train, X_test, y_train, y_test, epochs: int = 100, batch_size: int = 16):
        """ëª¨ë¸ í•™ìŠµ"""
        print("\n" + "="*70)
        print("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("="*70)
        print(f"ì—í¬í¬: {epochs}, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ì½œë°± ì„¤ì • (ë” ë¹ ë¥¸ ì¡°ê¸° ì¢…ë£Œ)
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=10,  # 15â†’10ìœ¼ë¡œ ê°ì†Œ
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,  # 5â†’3ìœ¼ë¡œ ê°ì†Œ
                min_lr=1e-7,
                verbose=1
            ),
            ModelCheckpoint(
                filepath=str(self.output_dir / "best_model.h5"),
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # í•™ìŠµ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"\ní•™ìŠµ ì™„ë£Œ!")
        print(f"  ìµœì¢… í•™ìŠµ ì •í™•ë„: {history.history['accuracy'][-1]*100:.2f}%")
        print(f"  ìµœì¢… ê²€ì¦ ì •í™•ë„: {history.history['val_accuracy'][-1]*100:.2f}%")
        print(f"  ìµœì¢… í•™ìŠµ ì†ì‹¤: {history.history['loss'][-1]:.4f}")
        print(f"  ìµœì¢… ê²€ì¦ ì†ì‹¤: {history.history['val_loss'][-1]:.4f}")
        
        # ê³¼ì í•© ê²½ê³ 
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        gap = train_acc - val_acc
        
        if gap > 0.15:
            print(f"\nâš ï¸  ê³¼ì í•© ê²½ê³ !")
            print(f"  í•™ìŠµ/ê²€ì¦ ì •í™•ë„ ì°¨ì´: {gap*100:.2f}%")
            print(f"  ì¶”ê°€ ì •ê·œí™” ë˜ëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif gap > 0.10:
            print(f"\nâš ï¸  ì•½ê°„ì˜ ê³¼ì í•© ê°ì§€")
            print(f"  í•™ìŠµ/ê²€ì¦ ì •í™•ë„ ì°¨ì´: {gap*100:.2f}%")
        else:
            print(f"\nâœ… ì ì ˆí•œ ì¼ë°˜í™” (ì°¨ì´: {gap*100:.2f}%)")
        
        return history
    
    def evaluate(self, model, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        print("\n" + "="*70)
        print("ëª¨ë¸ í‰ê°€")
        print("="*70)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test, verbose=0)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true_classes = np.argmax(y_test, axis=1)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(
            y_true_classes, 
            y_pred_classes,
            target_names=[self.int_to_label[i] for i in range(len(self.int_to_label))],
            digits=3
        ))
        
        # Confusion Matrix
        cm = confusion_matrix(y_true_classes, y_pred_classes)
        
        # ì •ê·œí™”ëœ confusion matrixë„ ê³„ì‚°
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # ë‘ ê°œì˜ confusion matrix ê·¸ë¦¬ê¸°
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        labels = [self.int_to_label[i] for i in range(len(self.int_to_label))]
        
        if HAS_SEABORN:
            # seaborn ì‚¬ìš©
            sns.heatmap(
                cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                xticklabels=labels,
                yticklabels=labels,
                ax=axes[0]
            )
            axes[0].set_title('Confusion Matrix (Counts)')
            axes[0].set_ylabel('True Label')
            axes[0].set_xlabel('Predicted Label')
            
            sns.heatmap(
                cm_normalized, 
                annot=True, 
                fmt='.2f', 
                cmap='Blues',
                xticklabels=labels,
                yticklabels=labels,
                ax=axes[1],
                vmin=0,
                vmax=1
            )
            axes[1].set_title('Confusion Matrix (Normalized)')
            axes[1].set_ylabel('True Label')
            axes[1].set_xlabel('Predicted Label')
        else:
            # matplotlibë§Œ ì‚¬ìš©
            im0 = axes[0].imshow(cm, cmap='Blues', aspect='auto')
            axes[0].set_title('Confusion Matrix (Counts)')
            axes[0].set_ylabel('True Label')
            axes[0].set_xlabel('Predicted Label')
            axes[0].set_xticks(range(len(labels)))
            axes[0].set_yticks(range(len(labels)))
            axes[0].set_xticklabels(labels, rotation=45, ha='right')
            axes[0].set_yticklabels(labels)
            
            # ìˆ«ì í‘œì‹œ
            for i in range(len(labels)):
                for j in range(len(labels)):
                    text = axes[0].text(j, i, int(cm[i, j]),
                                       ha="center", va="center", color="black" if cm[i, j] < cm.max()/2 else "white")
            plt.colorbar(im0, ax=axes[0])
            
            im1 = axes[1].imshow(cm_normalized, cmap='Blues', aspect='auto', vmin=0, vmax=1)
            axes[1].set_title('Confusion Matrix (Normalized)')
            axes[1].set_ylabel('True Label')
            axes[1].set_xlabel('Predicted Label')
            axes[1].set_xticks(range(len(labels)))
            axes[1].set_yticks(range(len(labels)))
            axes[1].set_xticklabels(labels, rotation=45, ha='right')
            axes[1].set_yticklabels(labels)
            
            # ìˆ«ì í‘œì‹œ
            for i in range(len(labels)):
                for j in range(len(labels)):
                    text = axes[1].text(j, i, f'{cm_normalized[i, j]:.2f}',
                                       ha="center", va="center", color="black" if cm_normalized[i, j] < 0.5 else "white")
            plt.colorbar(im1, ax=axes[1])
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        print(f"\nConfusion matrix ì €ì¥: {self.output_dir / 'confusion_matrix.png'}")
        plt.close()
        
        return y_pred, y_pred_classes, y_true_classes
    
    def plot_history(self, history):
        """í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss
        axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)
        axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
        axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].legend(fontsize=11)
        axes[0].grid(True, alpha=0.3)
        
        # ìµœì†Œê°’ í‘œì‹œ
        min_val_loss_epoch = np.argmin(history.history['val_loss'])
        min_val_loss = history.history['val_loss'][min_val_loss_epoch]
        axes[0].scatter([min_val_loss_epoch], [min_val_loss], color='red', s=100, zorder=5)
        axes[0].annotate(f'Best: {min_val_loss:.4f}', 
                        xy=(min_val_loss_epoch, min_val_loss),
                        xytext=(10, 10), textcoords='offset points',
                        fontsize=10, color='red')
        
        # Accuracy
        axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
        axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
        axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Epoch', fontsize=12)
        axes[1].set_ylabel('Accuracy', fontsize=12)
        axes[1].legend(fontsize=11)
        axes[1].grid(True, alpha=0.3)
        
        # ìµœëŒ€ê°’ í‘œì‹œ
        max_val_acc_epoch = np.argmax(history.history['val_accuracy'])
        max_val_acc = history.history['val_accuracy'][max_val_acc_epoch]
        axes[1].scatter([max_val_acc_epoch], [max_val_acc], color='green', s=100, zorder=5)
        axes[1].annotate(f'Best: {max_val_acc:.4f}', 
                        xy=(max_val_acc_epoch, max_val_acc),
                        xytext=(10, -20), textcoords='offset points',
                        fontsize=10, color='green')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'training_history.png', dpi=300, bbox_inches='tight')
        print(f"í•™ìŠµ ê³¡ì„  ì €ì¥: {self.output_dir / 'training_history.png'}")
        plt.close()
    
    def save_model(self, model):
        """ëª¨ë¸ ë° ì„¤ì • ì €ì¥"""
        print("\n" + "="*70)
        print("ëª¨ë¸ ì €ì¥")
        print("="*70)
        
        # ëª¨ë¸ ì €ì¥
        model_path = self.output_dir / "sign_language_model.h5"
        model.save(model_path)
        print(f"ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        model_info = {
            'label_to_int': self.label_to_int,
            'int_to_label': self.int_to_label,
            'max_sequence_length': self.max_sequence_length,
            'input_dim': self.input_dim,
            'num_classes': len(self.label_to_int),
            'scaler': self.scaler
        }
        
        info_path = self.output_dir / "model_info.pkl"
        with open(info_path, 'wb') as f:
            pickle.dump(model_info, f)
        print(f"ì„¤ì • ì €ì¥: {info_path}")
        
        # JSON í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (scaler ì œì™¸)
        json_info = {
            'label_to_int': self.label_to_int,
            'int_to_label': self.int_to_label,
            'max_sequence_length': int(self.max_sequence_length),
            'input_dim': int(self.input_dim),
            'num_classes': len(self.label_to_int),
            'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'labels': list(self.label_to_int.keys())
        }
        
        json_path = self.output_dir / "model_info.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_info, f, indent=2, ensure_ascii=False)
        print(f"JSON ì •ë³´ ì €ì¥: {json_path}")
        
        print("\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")


def main():
    parser = argparse.ArgumentParser(
        description="3D ì¢Œí‘œ ê¸°ë°˜ ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ (npy íŒŒì¼ + metadata.csv ì‚¬ìš©)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í•™ìŠµ
  python train_and_save_model_v6.py --data-dir coordinates_output
  
  # ë°ì´í„° ì¦ê°• ì ìš© (ê¶Œì¥)
  python train_and_save_model_v6.py --data-dir coordinates_output --augment
  
  # ì—í¬í¬ ë° ë°°ì¹˜ í¬ê¸° ì¡°ì •
  python train_and_save_model_v6.py --data-dir coordinates_output --epochs 150 --batch-size 8
  
  # ì €ì¥ ê²½ë¡œ ì§€ì •
  python train_and_save_model_v6.py --data-dir coordinates_output --output-dir my_model

ë°ì´í„° êµ¬ì¡°:
  coordinates_output/
    â”œâ”€â”€ metadata.csv
    â”œâ”€â”€ L_001_keypoints.npy
    â”œâ”€â”€ L_002_keypoints.npy
    â””â”€â”€ ...
        """
    )
    
    parser.add_argument("--data-dir", "-d", required=True, type=str,
                       help="npy íŒŒì¼ ë° metadata.csvê°€ ìˆëŠ” í´ë” ê²½ë¡œ")
    parser.add_argument("--output-dir", "-o", default="trained_model/trained_model_v6",
                       help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: trained_model/trained_model_v6)")
    parser.add_argument("--epochs", "-e", type=int, default=100,
                       help="í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸: 100)")
    parser.add_argument("--batch-size", "-b", type=int, default=16,
                       help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 16)")
    parser.add_argument("--augment", "-a", action="store_true",
                       help="ë°ì´í„° ì¦ê°• í™œì„±í™” (3ë°° ì¦ê°€)")
    
    args = parser.parse_args()
    
    print("="*70)
    print("ìˆ˜ì–´ ì¸ì‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*70)
    print(f"ë°ì´í„° ê²½ë¡œ: {args.data_dir}")
    print(f"ì €ì¥ ê²½ë¡œ: {args.output_dir}")
    print(f"ì—í¬í¬: {args.epochs}")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"ë°ì´í„° ì¦ê°•: {'í™œì„±í™”' if args.augment else 'ë¹„í™œì„±í™”'}")
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = SignLanguageTrainer(args.data_dir, args.output_dir)
        
        # ë°ì´í„° ë¡œë“œ
        sequences, labels = trainer.load_data(augment=args.augment)
        
        # ì „ì²˜ë¦¬
        X_train, X_test, y_train, y_test = trainer.preprocess_data(sequences, labels)
        
        # ëª¨ë¸ êµ¬ì¶•
        model = trainer.build_model()
        
        # í•™ìŠµ
        history = trainer.train(model, X_train, X_test, y_train, y_test, 
                               epochs=args.epochs, batch_size=args.batch_size)
        
        # í‰ê°€
        trainer.evaluate(model, X_test, y_test)
        
        # ê²°ê³¼ ì‹œê°í™”
        trainer.plot_history(history)
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model(model)
        
        print("\n" + "="*70)
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print("="*70)
        print(f"\nì €ì¥ëœ íŒŒì¼:")
        print(f"  ğŸ“ {args.output_dir}/")
        print(f"    â”œâ”€â”€ sign_language_model.h5")
        print(f"    â”œâ”€â”€ best_model.h5")
        print(f"    â”œâ”€â”€ model_info.pkl")
        print(f"    â”œâ”€â”€ model_info.json")
        print(f"    â”œâ”€â”€ training_data_info.csv")
        print(f"    â”œâ”€â”€ confusion_matrix.png")
        print(f"    â””â”€â”€ training_history.png")
        print(f"\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
        print(f"  python 3d_predict_sign_language.py --video test.mp4 --model-dir {args.output_dir}")
        
    except FileNotFoundError as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()