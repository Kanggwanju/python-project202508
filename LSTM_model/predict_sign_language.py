"""
ìƒˆë¡œìš´ ìˆ˜ì–´ ì˜ìƒì„ ì…ë ¥ë°›ì•„ í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
python predict_sign_language.py --video "test_video.mp4"
python predict_sign_language.py --video "test_video.mp4" --show-probs
"""

import cv2
import mediapipe as mp
import numpy as np
import pickle
import argparse
from pathlib import Path
from tensorflow.keras.models import load_model

class SignLanguagePredictor:
    def __init__(self, model_dir="trained_model"):
        """
        í•™ìŠµëœ ëª¨ë¸ê³¼ ì„¤ì • ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        model_path = Path(model_dir) / "sign_language_model.h5"
        info_path = Path(model_dir) / "model_info.pkl"
        
        if not model_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        if not info_path.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {info_path}")
        
        self.model = load_model(model_path)
        
        with open(info_path, 'rb') as f:
            model_info = pickle.load(f)
        
        self.label_to_int = model_info['label_to_int']
        self.int_to_label = model_info['int_to_label']
        self.max_sequence_length = model_info['max_sequence_length']
        self.input_dim = model_info['input_dim']
        self.num_classes = model_info['num_classes']
        self.scaler = model_info['scaler']
        
        # MediaPipe Holistic ì´ˆê¸°í™”
        self.mp_holistic = mp.solutions.holistic
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.pose_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
        
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"í•™ìŠµëœ ìˆ˜ì–´ ë‹¨ì–´ ({self.num_classes}ê°œ): {list(self.label_to_int.keys())}")
    
    def extract_keypoints_from_video(self, video_path: str, frame_skip: int = 2):
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        original_fps = cap.get(cv2.CAP_PROP_FPS)
        
        # 30fps ëª©í‘œë¡œ frame_skip ì¡°ì •
        if original_fps > 30:
            auto_skip = int(original_fps / 30)
            frame_skip = max(frame_skip, auto_skip)
        
        frames_data = []
        frame_count = 0
        extracted = 0
        
        print(f"ì˜ìƒ ì •ë³´: {total_frames}í”„ë ˆì„, {original_fps:.1f}fps")
        print(f"í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ì¤‘...", end="", flush=True)
        
        while True:
            success, frame = cap.read()
            if not success:
                break
            
            if frame_count % frame_skip == 0:
                coords = self._process_frame(frame)
                if coords:
                    frames_data.append(coords)
                    extracted += 1
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if extracted % 10 == 0:
                        print(".", end="", flush=True)
            
            frame_count += 1
        
        cap.release()
        print(f" ì™„ë£Œ! ({extracted}í”„ë ˆì„ ì¶”ì¶œ)")
        
        if len(frames_data) == 0:
            raise ValueError("í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒì— ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return np.array(frames_data)
    
    def _process_frame(self, frame: np.ndarray) -> list:
        """
        ë‹¨ì¼ í”„ë ˆì„ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image_rgb.flags.writeable = False
        results = self.holistic.process(image_rgb)
        
        if not hasattr(results, 'pose_landmarks') or results.pose_landmarks is None:
            return []
        
        keypoints = []
        
        # Pose (15ê°œ)
        for idx in self.pose_indices:
            lm = results.pose_landmarks.landmark[idx]
            keypoints.append([float(lm.x), float(lm.y)])
        
        # Left Hand (21ê°œ)
        if hasattr(results, 'left_hand_landmarks') and results.left_hand_landmarks:
            for lm in results.left_hand_landmarks.landmark:
                keypoints.append([float(lm.x), float(lm.y)])
        else:
            keypoints.extend([[0.0, 0.0]] * 21)
        
        # Right Hand (21ê°œ)
        if hasattr(results, 'right_hand_landmarks') and results.right_hand_landmarks:
            for lm in results.right_hand_landmarks.landmark:
                keypoints.append([float(lm.x), float(lm.y)])
        else:
            keypoints.extend([[0.0, 0.0]] * 21)
        
        return keypoints
    
    def preprocess_sequence(self, keypoints: np.ndarray):
        """
        ì¶”ì¶œëœ í‚¤í¬ì¸íŠ¸ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        # í‰íƒ„í™”
        flattened = keypoints.reshape(keypoints.shape[0], -1)
        
        # ìŠ¤ì¼€ì¼ë§ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©)
        scaled = self.scaler.transform(flattened)
        
        # íŒ¨ë”©
        if scaled.shape[0] >= self.max_sequence_length:
            padded = scaled[:self.max_sequence_length]
        else:
            padding = np.zeros((self.max_sequence_length - scaled.shape[0], scaled.shape[1]))
            padded = np.vstack((scaled, padding))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        return np.expand_dims(padded, axis=0)
    
    def predict(self, video_path: str, show_probabilities: bool = False):
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ìˆ˜ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        """
        print(f"\n{'='*60}")
        print(f"ì˜ˆì¸¡ ì‹œì‘: {video_path}")
        print(f"{'='*60}\n")
        
        # 1. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
        keypoints = self.extract_keypoints_from_video(video_path)
        
        # 2. ì „ì²˜ë¦¬
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        preprocessed = self.preprocess_sequence(keypoints)
        
        # 3. ì˜ˆì¸¡
        print("ì˜ˆì¸¡ ì¤‘...")
        prediction = self.model.predict(preprocessed, verbose=0)
        
        # 4. ê²°ê³¼ ë¶„ì„
        predicted_class_int = np.argmax(prediction[0])
        predicted_label = self.int_to_label[predicted_class_int]
        confidence = prediction[0][predicted_class_int] * 100
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ğŸ“Œ ì˜ˆì¸¡ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"ì˜ˆì¸¡ëœ ìˆ˜ì–´: {predicted_label}")
        print(f"ì‹ ë¢°ë„: {confidence:.2f}%")
        
        if show_probabilities:
            print(f"\n{'â”€'*60}")
            print(f"ì „ì²´ í™•ë¥  ë¶„í¬:")
            print(f"{'â”€'*60}")
            
            # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_indices = np.argsort(prediction[0])[::-1]
            for idx in sorted_indices:
                label = self.int_to_label[idx]
                prob = prediction[0][idx] * 100
                bar_length = int(prob / 2)  # 50% = 25ì¹¸
                bar = 'â–ˆ' * bar_length + 'â–‘' * (50 - bar_length)
                print(f"{label:15s} â”‚ {bar} â”‚ {prob:6.2f}%")
        
        print(f"{'='*60}\n")
        
        return {
            'predicted_label': predicted_label,
            'confidence': confidence,
            'all_probabilities': {self.int_to_label[i]: float(prediction[0][i] * 100) 
                                 for i in range(len(prediction[0]))}
        }


def main():
    parser = argparse.ArgumentParser(
        description="í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ìˆ˜ì–´ ì˜ìƒì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì˜ˆì¸¡
  python predict_sign_language.py --video "test_video.mp4"
  
  # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥  í‘œì‹œ
  python predict_sign_language.py --video "test_video.mp4" --show-probs
  
  # ë‹¤ë¥¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
  python predict_sign_language.py --video "test.mp4" --model-dir "my_model"
        """
    )
    
    parser.add_argument("--video", "-v", required=True,
                       help="ì˜ˆì¸¡í•  mp4 ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model-dir", "-m", default="trained_model",
                       help="í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: trained_model)")
    parser.add_argument("--show-probs", "-p", action="store_true",
                       help="ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ í‘œì‹œ")
    parser.add_argument("--frame-skip", "-s", type=int, default=2,
                       help="í”„ë ˆì„ ê±´ë„ˆë›°ê¸° (ê¸°ë³¸: 2)")
    
    args = parser.parse_args()
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.video).exists():
        print(f"âŒ ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.video}")
        return
    
    try:
        # ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
        predictor = SignLanguagePredictor(model_dir=args.model_dir)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        result = predictor.predict(args.video, show_probabilities=args.show_probs)
        
    except FileNotFoundError as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        print("\në¨¼ì € train_and_save_model.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()